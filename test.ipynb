{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a92f6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, cv2\n",
    "import numpy as np\n",
    "from cotracker.utils.visualizer import Visualizer, read_video_from_path\n",
    "from IPython.display import HTML, display\n",
    "from base64 import b64encode\n",
    "from cotracker.predictor import CoTrackerPredictor\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json, subprocess, labelme\n",
    "import PIL.Image as Image\n",
    "from labelme import utils\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "model = CoTrackerPredictor(\n",
    "    checkpoint=os.path.join(\n",
    "        './checkpoints/scaled_offline.pth'\n",
    "    )\n",
    ")\n",
    "video = read_video_from_path(\"videos/video2.mp4\")\n",
    "video = torch.from_numpy(video).permute(0, 3, 1, 2)[None].float()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    video = video.cuda()\n",
    "\n",
    "mask_laparoscope = np.load('c:/github/occlusions/ischemia_models_pigs/laparoscope_masks/pig1_bowel_baseline.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab22b602",
   "metadata": {},
   "source": [
    "# Select points and track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a05f009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 1, 2])\n",
      "torch.Size([1, 150, 1, 2])\n",
      "torch.Size([1, 200, 1, 2])\n",
      "torch.Size([1, 250, 1, 2])\n",
      "torch.Size([1, 300, 1, 2])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     65\u001b[39m         pred_visibility = torch.cat([pred_visibility, chunk_pred_visibility], dim=\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m pred_tracks = pred_tracks[:,:video.shape[\u001b[32m1\u001b[39m], :, :]\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m pred_visibility = \u001b[43mpred_visibility\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     70\u001b[39m vis = Visualizer(\n\u001b[32m     71\u001b[39m     save_dir=\u001b[33m'\u001b[39m\u001b[33m./videos\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     72\u001b[39m     linewidth=\u001b[32m3\u001b[39m,\n\u001b[32m     73\u001b[39m     mode=\u001b[33m'\u001b[39m\u001b[33mcool\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     74\u001b[39m     tracks_leave_trace=-\u001b[32m1\u001b[39m\n\u001b[32m     75\u001b[39m )\n\u001b[32m     76\u001b[39m vis.visualize(\n\u001b[32m     77\u001b[39m     video=video,\n\u001b[32m     78\u001b[39m     tracks=pred_tracks,\n\u001b[32m     79\u001b[39m     visibility=pred_visibility,\n\u001b[32m     80\u001b[39m     filename=\u001b[33m'\u001b[39m\u001b[33mqueries\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: too many indices for tensor of dimension 3"
     ]
    }
   ],
   "source": [
    "video = video[:,0:300,:,:,:]\n",
    "\n",
    "#select a point, and the model will track it\n",
    "def select_points(frame):\n",
    "    frame = frame.transpose(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
    "    frame = frame.astype(np.uint8)    # Convert to uint8 data type\n",
    "    # RGB to BGR\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    selected_points = []\n",
    "\n",
    "    # Callback to capture the selected points\n",
    "    def mouse_callback(event, x, y, flags, param):\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            cv2.circle(frame, (x, y), 5, (0, 255, 0), -1)\n",
    "            selected_points.append((x, y))\n",
    "\n",
    "    cv2.setMouseCallback('frame', mouse_callback)\n",
    "\n",
    "    while True:\n",
    "        cv2.imshow('frame', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Exit on pressing 'q'\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    queries = []\n",
    "    for point in selected_points:\n",
    "        queries.append([0.0, point[0], point[1]])  # Format: [0., y, x]\n",
    "    \n",
    "    queries = torch.tensor(queries)\n",
    "    if torch.cuda.is_available():\n",
    "        queries = queries.cuda()\n",
    "    return queries\n",
    "\n",
    "queries = select_points(video[0][0].cpu().numpy())\n",
    "\n",
    "chunk_size = 50  # Number of frames per chunk\n",
    "pred_tracks = None\n",
    "pred_visibility = None\n",
    "for i in range(0, video.shape[1], chunk_size):\n",
    "\n",
    "    if i == 0:\n",
    "        queries = queries\n",
    "    else:\n",
    "        queries = chunk_pred_tracks[0, -1,:,:]\n",
    "        queries = torch.cat([torch.tensor([[0]], device=queries.device), queries], dim=1)\n",
    "    video_chunk = video[:, i:i+chunk_size, :, :]\n",
    "\n",
    "    # Pad the last chunk if it has fewer frames\n",
    "    if video_chunk.shape[1] < chunk_size:\n",
    "        padding = (0, 0, 0, 0, 0, chunk_size - video_chunk.shape[1])  # Pad along dim=1\n",
    "        video_chunk = F.pad(video_chunk, padding, mode='constant', value=0)\n",
    "        chunk_pred_tracks = F.pad(chunk_pred_tracks, (0, 0, 0, 0, 0, chunk_size - chunk_pred_tracks.shape[1]), mode='constant', value=0)\n",
    "        chunk_pred_visibility = F.pad(chunk_pred_visibility, (0, chunk_size - chunk_pred_visibility.shape[1]), mode='constant', value=0)\n",
    "    else:\n",
    "        chunk_pred_tracks, chunk_pred_visibility = model(video_chunk, queries=queries[None])\n",
    "\n",
    "    if pred_tracks is None:\n",
    "        pred_tracks = chunk_pred_tracks\n",
    "        pred_visibility = chunk_pred_visibility\n",
    "    else:\n",
    "        pred_tracks = torch.cat([pred_tracks, chunk_pred_tracks], dim=1)\n",
    "        pred_visibility = torch.cat([pred_visibility, chunk_pred_visibility], dim=1)\n",
    "\n",
    "pred_tracks = pred_tracks[:,:video.shape[1], :, :]\n",
    "pred_visibility = pred_visibility[:, :video.shape[1],:]\n",
    "\n",
    "vis = Visualizer(\n",
    "    save_dir='./videos',\n",
    "    linewidth=3,\n",
    "    mode='cool',\n",
    "    tracks_leave_trace=-1\n",
    ")\n",
    "vis.visualize(\n",
    "    video=video,\n",
    "    tracks=pred_tracks,\n",
    "    visibility=pred_visibility,\n",
    "    filename='queries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15f1f366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 300, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_visibility.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d08f6",
   "metadata": {},
   "source": [
    "# Track a grid of points along with a segmentation mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c8b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get mask\n",
    "def mask_using_labelme(rgb):\n",
    "\n",
    "    #delete previous files\n",
    "    if os.path.exists('image_to_annotate.jpg'):\n",
    "        os.remove('image_to_annotate.jpg')\n",
    "    if os.path.exists('labelme.json'):\n",
    "        os.remove('labelme.json')\n",
    "')\n",
    "    \n",
    "    image = Image.fromarray(rgb)\n",
    "    image.save('image_to_annotate.jpg')\n",
    "    subprocess.run(['labelme', 'image_to_annotate.jpg'])\n",
    "\n",
    "    # run labelme in terminal and annotate. Save annotations to json file\n",
    "    with open('labelme.json') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    img = utils.img_b64_to_arr(data['imageData'])\n",
    "\n",
    "    label_name_to_value = {shape['label']: i + 1 for i, shape in enumerate(data['shapes'])}\n",
    "    lbl, _ = utils.shapes_to_label(img.shape, data['shapes'], label_name_to_value=label_name_to_value)\n",
    "    return lbl.astype(np.uint8)\n",
    "\n",
    "mask = mask_using_labelme(video[0][0].cpu().numpy().transpose(1, 2, 0).astype(np.uint8))  \n",
    "plt.imshow((mask[...,None]*video[0,0].permute(1,2,0).cpu().numpy()/255.)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd47845",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 30\n",
    "pred_tracks, pred_visibility = model(video, grid_size=grid_size, segm_mask=torch.from_numpy(mask)[None, None])\n",
    "vis = Visualizer(\n",
    "    save_dir='./videos',\n",
    "    pad_value=100,\n",
    "    linewidth=2,\n",
    ")\n",
    "vis.visualize(\n",
    "    video=video,\n",
    "    tracks=pred_tracks,\n",
    "    visibility=pred_visibility,\n",
    "    filename='segm_grid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "co-tracker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
